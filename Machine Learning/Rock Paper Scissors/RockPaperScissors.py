# -*- coding: utf-8 -*-
"""Proyek Akhir: Klasifikasi Gambar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rh_UvRWCxipuB1XKPn4HVCQzjSbIajYZ

Nama: Elin Betsey Br Ginting
"""

import tensorflow as tf

!wget --no-check-certificate \
  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip

import zipfile,os
local_zip = '/content/rockpaperscissors.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

import os
import shutil
import random
import math

# Path to the main directory containing three folders (paper, rock, scissors)
main_dir = '/tmp/rockpaperscissors'

# Output directories for training and validation data
train_dir = '/tmp/rockpaperscissors/train'
valid_dir = '/tmp/rockpaperscissors/valid'

# Percentage of data for training (60%)
train_percentage = 0.6

# Remove existing train and valid directories if they exist
shutil.rmtree(train_dir, ignore_errors=True)
shutil.rmtree(valid_dir, ignore_errors=True)

# Create output directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(valid_dir, exist_ok=True)

# Iterate over each category (paper, rock, scissors)
for category in ['paper', 'rock', 'scissors']:
    category_dir = os.path.join(main_dir, category)

    # Get a list of all images in the current category
    all_images = os.listdir(category_dir)

    # Calculate the number of images for training and validation
    num_images = len(all_images)
    num_train = math.ceil(num_images * train_percentage)  # menggunakan math.ceil untuk pembulatan ke atas

    # Randomly shuffle the list of images
    random.shuffle(all_images)

    # Split the images into training and validation sets
    train_images = all_images[:num_train]
    valid_images = all_images[num_train:]

    # Create output directories for each category in training and validation
    train_category_dir = os.path.join(train_dir, category)
    valid_category_dir = os.path.join(valid_dir, category)

    os.makedirs(train_category_dir, exist_ok=True)
    os.makedirs(valid_category_dir, exist_ok=True)

    # Copy training images to the appropriate directory
    for image in train_images:
        src_path = os.path.join(category_dir, image)
        dest_path = os.path.join(train_category_dir, image)
        shutil.copy(src_path, dest_path)

    # Copy validation images to the appropriate directory
    for image in valid_images:
        src_path = os.path.join(category_dir, image)
        dest_path = os.path.join(valid_category_dir, image)
        shutil.copy(src_path, dest_path)

print("Data splitting and copying complete.")

import shutil

# Ganti path dengan direktori yang ingin Anda hapus
directory_to_delete = '/tm/rockpaperscissors.zip'

# Menghapus direktori dan isinya
shutil.rmtree(directory_to_delete, ignore_errors=True)

print(f"Directory {directory_to_delete} and its contents have been deleted.")

import os

# Ganti path dengan direktori yang ingin Anda cek
directory_to_check = '/tmp/rockpaperscissors/train'

# Mendapatkan list isi direktori
directory_contents = os.listdir(directory_to_check)

# Menampilkan isi direktori
print(f"Isi dari {directory_to_check} adalah:")
for item in directory_contents:
    print(item)

os.listdir('/tmp/rockpaperscissors/train')

import os
from PIL import Image

def check_unreadable_images(directory):
    unreadable_images = []

    for root, dirs, files in os.walk(directory):
        for file in files:
            file_path = os.path.join(root, file)

            try:
                with Image.open(file_path) as img:
                    img.verify()
            except (IOError, SyntaxError) as e:
                unreadable_images.append(file_path)

    return unreadable_images

# Ganti 'your_directory_path' dengan path direktori yang ingin Anda periksa
directory_to_check = '/tmp/rockpaperscissors/train/scissors'
unreadable_files = check_unreadable_images(directory_to_check)

print("Unreadable files:")
for file_path in unreadable_files:
    print(file_path)

from tensorflow.keras.preprocessing.image import ImageDataGenerator


train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    width_shift_range=0.2,  # Geser lebar
                    height_shift_range=0.2,  # Geser tinggi
                    horizontal_flip=True,
                    vertical_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

test_datagen = ImageDataGenerator(
                    rescale=1./255)

augmented_data = train_datagen.flow_from_directory('/tmp/rockpaperscissors/train',
                                             target_size=(150, 150),
                                             batch_size=32,
                                             class_mode='categorical')

train_generator = train_datagen.flow_from_directory(
        train_dir,  # direktori data latih
        target_size=(150, 150),  # mengubah resolusi seluruh gambar menjadi 150x150 piksel
        batch_size=4,
        class_mode='categorical', classes=['paper', 'rock', 'scissors'])

validation_generator = test_datagen.flow_from_directory(
        valid_dir, # direktori data validasi
        target_size=(150, 150), # mengubah resolusi seluruh gambar menjadi 150x150 piksel
        batch_size=4,
        class_mode='categorical', classes=['paper', 'rock', 'scissors'])

print("Training Generator:")
print(len(train_generator.filenames))
print("Validation Generator:")
print(len(validation_generator.filenames))



#membangun model Convolutional Neural Network (CNN) menggunakan keras
model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(filters=64,
                              kernel_size=3,
                              activation="relu",
                              input_shape=(150, 150, 3)),
        tf.keras.layers.Conv2D(32, 3, activation="relu"),
        tf.keras.layers.MaxPool2D(pool_size=2,
                                  padding="valid"),
        tf.keras.layers.Conv2D(32, 3, activation="relu"),
        tf.keras.layers.Conv2D(32, 3, activation="relu"),
        tf.keras.layers.MaxPool2D(2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dense(128, activation="relu"),
        tf.keras.layers.Dense(3, activation='softmax')
])



model.summary()

from tensorflow.keras.callbacks import Callback

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.96 and logs.get('val_accuracy')>0.96):
      print("\nAkurasi telah mencapai >97%!")
      self.model.stop_training= True
callbacks = myCallback()

model.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

model.fit(
      train_generator,
      steps_per_epoch=50,  # berapa batch yang akan dieksekusi pada setiap epoch
      epochs=100, # tambahkan epochs jika akurasi model belum optimal
      validation_data=validation_generator, # menampilkan akurasi pengujian data validasi
      callbacks = [callbacks],
      validation_steps=5,  # berapa batch yang akan dieksekusi pada setiap epoch
      verbose=2)

import os

model_dir = "/tmp/rockpaperscissors/model"
saved_models = [f for f in os.listdir(model_dir) if f.endswith(".h5")]

print("List of saved models:")
for saved_model in saved_models:
    print(saved_model)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from google.colab import files
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from tensorflow.keras.models import load_model

# %matplotlib inline

uploaded = files.upload()

for fn in uploaded.keys():
    # predicting images
    path = fn
    img = image.load_img(path, target_size=(150, 150))

    imgplot = plt.imshow(img)
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    images = np.vstack([x])

    classes = model.predict(images, batch_size=10)[0]

    # Convert the output probabilities to class labels
    class_labels = ['paper', 'rock', 'scissors']
    predicted_class = class_labels[np.argmax(classes)]

    print(fn)
    print(predicted_class)